<!doctype html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7" lang=""> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8" lang=""> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9" lang=""> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang=""> <!--<![endif]-->
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title></title>
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link rel="apple-touch-icon" href="apple-touch-icon.png">

        <link rel="stylesheet" href="css/bootstrap.min.css">
        <style>
            body {
                padding-top: 50px;
                padding-bottom: 20px;
            }
        </style>
        <link rel="stylesheet" href="css/main.css">

        <script src="js/vendor/modernizr-2.8.3-respond-1.4.2.min.js"></script>
    </head>
    <body>
        <!--[if lt IE 8]>
            <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</p>
        <![endif]-->
    <nav class="navbar navbar-fixed-top" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="#">Facebook AI Research</a>
        </div>
      </div>
    </nav>

    <div class="col-md-12___1" style="width: 100%">
      <div class="dtitle">
	<h1 class="mtitle"> The Eyescream Project </h1>
	<h2 class="mtitle2"><i> NeuralNets dreaming natural images </i></h2>
	<h2 class="mtitle3"> - <a href="http://soumith.ch">Soumith</a>, 
	  <a href="http://www.cs.nyu.edu/~denton">Emily</a>, 
	  <a href="http://www.sci.ccny.cuny.edu/~szlam/">Arthur & 
	    <a href="https://research.facebook.com/researchers/1522313361316754/rob-fergus/">Rob</a> 
	</h2>
      </div>
      <canvas id="screamer" style="border:1px solid #000000;"></canvas>
    </div>

    <div class="container">
      <!-- Example row of columns -->
      <div class="row">
	<div class="col-md-8">
	  <p> * Some of the images above are being generated live in the browser, 
	    and are unique to you and only you </p>
	  <blockquote><p>Making a machine generate (dream, hallucinate, ...) natural images 
	    is a lot of fun, and quite complicated. </p></blockquote>

	  <h1> The lead up </h1>
	  <p> 
	    About two months ago, Emily, Arthur, Rob and I decided to try something that's been 
	    at the back of our heads since <i> Ian Goodfellow </i> first presented (the awesome) 
	    Generative Adversarial Networks at NIPS last year. Ian's work came close to generating 
	    natural-ish images, but still fell short in terms of image quality. 
	    The paper excited us, and we discussed it to length at the conference and after.
	  </p>
	  <br/>
	  <p class='lead'> Adversarial networks are fun things. </p>
	  <p>
	    You pit a generative (G) machine against a discriminative (G) machine and make them fight. 
	    The G machine attempts to produce fake data that looks so real that the D machine cannot tell it apart. 
	    The D machine is learning to not get fooled by the G machine. 
	    As these both play their mini-max game, you hope to get a good quality generator, 
	    that can generate infinite samples of images that look real.
	  </p>
	  <div class="thumbnail with-caption">
	    <img src="https://upload.wikimedia.org/wikipedia/en/4/46/Street_Fighter_II_%28arcade%29_screenshot.png"
		 class="img-responsive">
	    <p>Street Fighter as an analogy for Adversarial Nets. <br/>Two networks fight against each other, 
	      and slowly get better at the game after playing long enough</p>
	  </div>
	  
	  <h1> The research process </h1>
	  <h3> First steps: making Goodfellow's neural network bigger and deeper </h3>
	  <p> To keep this section brief, this did not work. It didn't work at all.
	    The adversarial training was harder to be stable, and when we did get a stable model,
	    it did not improve image sample quality.
	  </p>

	  <h3> Next try: multi-scale generators </h3>
	  <p> The key idea that got us going was to train a laplacian scale of networks, 
	    each network predicting one scale ahead. 

	    What that means in simpler terms is that, the network at a particular scale gets 
	    the coarse-scaled version of the image to predict, and generates the finer-scale version of the image.
	  </p>
	  <div class="thumbnail with-caption">
	    <img src="images/LAPGAN.png"
		 class="img-responsive">
	    <p>Each network Gx generates the difference image, when added to the input image, 
	      gives a finer-scaled image.</p>
	  </div>
	  <div class="thumbnail with-caption">
	    <img src="images/bedroom1.png"
		 class="img-responsive">
	    <p>Who in their right mind works on generating bedrooms?</p>
	  </div>
	  <div class="thumbnail with-caption">
	    <img src="images/predicted16.png"
		 class="img-responsive">
	    <p>Skylines and bridges</p>
	  </div>
	  <h4> The illusion of objectness </h4>
	  <p>
	    The multi-scale generation worked extremely well. It got us to a point where we had samples that looked 
	    much more real than previous attempts at generating such samples. However, there was one problem.
	  </p>
	  <blockquote>
	  <p>
	    The images always looked object-like, but had no actual object.
	  </p>
	  </blockquote>
	  <p>
	    Looking at the image at a glance would totally wow you, but looking closer, 
	    you'd be scratching your head.

	    This wasn't going as well as we'd hoped, we needed another advance to get us 
	    closer to generating objects that actually were objects.
	  </p>
	  
	  <h4> Some fun! </h4>
	  <p>
	    Because our neural networks are convolutional in nature, we can recursively apply a 
	    particular generator over and over again to get images as large as we want. 
	    However, image statics vary quite a bit at each scale, especially in the lower octaves. 
	    You can also chain different generators trained on different datasets together. For example,
	    you can train a generator on a cats dataset, and chain it in the next scale with a generator
	    trained on horses. It would be fun to try that.
	  </p>
	  <div class="thumbnail with-caption">
	    <img src="images/recursive.png"
		 class="img-responsive">
	    <p>256x256 images generated recursively. The network has never learnt to generate beyond 
	      32x32 pixels, so you see some watercolor effects :)</p>
	  </div>

	  <h3> Finally: Class-conditional, multi-scale generators </h3>
	  <p>
	    Our last idea, which was a fairly natural extension for us to enforce explicit objectness, 
	    was to pass to the G networks not only an input random noise, but also a one-hot coding of the
	    class label we want to generate. 
	  </p>
	    <div class="thumbnail with-caption">
	      <img src="images/ccgan.png"
		   class="img-responsive">
	      <p>Class-conditional generative adversarial network</p>
	    </div>
	    <p>
	    This idea finally nailed it home in terms of objectness of our generated output.
	    We trained in on the CIFAR-10 database to produce cool looking horses, cars, airplanes etc.
	    </p>
	  <div class="thumbnail with-caption">
	    <img src="images/cifar_samples_classcond_28x28.png"
		 class="img-responsive">
	    <p>Class-conditional CIFAR-10 28x28 generations</p>
	  </div>
	  <p>
	    Another simplification that we did for training the LSUN database (because it is large enough, 
	    and because we were running against a conference deadline)  is that we trained a separate network 
	    for each class. That also produced nice towers, bedrooms, churches and conference rooms.
	  </p>
	  <div class="thumbnail with-caption">
	    <img src="images/church_outdoor.png"
		 class="img-responsive">
	    <p>Generated Church Outdoors of 64x64 pixels in size</p>
	  </div>

	  <h1> Checking network cheating </h1>
	  <p> One of the easiest thing a generative network can do to generate realistic samples is to
	    simply copy the training images. This is not a true generative model that generalizes to 
	    all natural images, but one that overfit heavily on the training set.

	    This is thing we meticulously checked while training our networks, i.e. that our network is simply
	    not copying the training samples.

	    We've done this in a couple of ways.
	  </p>
	  <h4> Nearest neighbors using L2 pixel-wise distance </h4>
	  <p>
	    For each of our generated sample, we found it's nearest neighbor in the training set using a 
	    L2 distance in pixel space.
	  </p>
	  <h4> Nearest neighbors using L2 feature-space distance </h4>
	  <p>
	    For each of our generated sample, we found it's nearest neighbor in the training set using a 
	    L2 distance in feature-space using 
	    <a href="https://gist.github.com/mavenlin/e56253735ef32c3c296d">
	      this well-performing Network-in-Network CIFAR model
	    </a>.
	  </p>
	  <p> Both the experiments comfortably concluded that our networks are generating images that are not
	    just trivial manipulations of copied training images.
	  </p>
	  <h1> A few last details </h1>
	  <p>
	    <ul>
	      <li>
		Adversarial training was hard to get right. 
		Changing the model a little bit, or changing the learning rates a little bit made 
		the network not learn anything at all.
	      </li>
	      <li>
		Using batch normalization in the discriminative network makes the whole adversarial training
		not work.
	      </li>
	      <li>
		We developed a few thumb rules like keeping the total number of parameters in the D network 
		to be a 0.5 or 0.125 factor of the total number of parameters in the G network.
	      </li>
	    </ul>
	  </p>
	  <h1> Where do we go from here? </h1>
	  <p> There are so many things to explore, as follow-up work to this paper. A list of simple ideas would be:
	    <ul>
	      <li> instead of one-hot coding, give word-embeddings as the conditional vector. 
		Imagine the awesomeness of going from image captions to images.
	      </li>
	      <li>
		Apply the same exact method to generate audio and video
	      </li>
	      <li>
		Develop better theory and more stabilized training of adversarial nets.
	      </li>
	    </ul>
	  </p>
	  <h1> Code etc. </h1>
	  <p>
	    We will release the Torch code to train all of our models by the end of the week.
	    Come back soon to this section and get the code.
	  </p>
	</div>
      </div>
      <hr>

      <div id="disqus_thread"></div>
      <script type="text/javascript">
	/* * * CONFIGURATION VARIABLES * * */
	var disqus_shortname = 'eyescreamfb';
	
	/* * * DON'T EDIT BELOW THIS LINE * * */
	(function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	})();
      </script>
      <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
      <footer>
        <p>&copy; Facebook 2015</p>
      </footer>
    </div> <!-- /container -->
    <script src="//ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
    <script>
      window.jQuery || document.write('<script src="js/vendor/jquery-1.11.2.min.js"><\/script>')
      </script>
      <script src="js/vendor/bootstrap.min.js"></script>
      <script src="js/nn.js"></script>
      <script src="js/main.js"></script>
    </body>
</html>
